[{"text": "Today I'm going to talk\nabout technology and society.", "start": 12.82, "duration": 4.08}, {"text": "The Department of Transport\nestimated that last year", "start": 18.86, "duration": 3.696}, {"text": "35,000 people died\nfrom traffic crashes in the US alone.", "start": 22.58, "duration": 4.08}, {"text": "Worldwide, 1.2 million people\ndie every year in traffic accidents.", "start": 27.86, "duration": 4.8}, {"text": "If there was a way we could eliminate\n90 percent of those accidents,", "start": 33.58, "duration": 4.096}, {"text": "would you support it?", "start": 37.7, "duration": 1.2}, {"text": "Of course you would.", "start": 39.54, "duration": 1.296}, {"text": "This is what driverless car technology\npromises to achieve", "start": 40.86, "duration": 3.655}, {"text": "by eliminating the main\nsource of accidents --", "start": 44.54, "duration": 2.816}, {"text": "human error.", "start": 47.38, "duration": 1.2}, {"text": "Now picture yourself\nin a driverless car in the year 2030,", "start": 49.74, "duration": 5.416}, {"text": "sitting back and watching\nthis vintage TEDxCambridge video.", "start": 55.18, "duration": 3.456}, {"text": "(Laughter)", "start": 58.66, "duration": 2.0}, {"text": "All of a sudden,", "start": 61.34, "duration": 1.216}, {"text": "the car experiences mechanical failure\nand is unable to stop.", "start": 62.58, "duration": 3.28}, {"text": "If the car continues,", "start": 67.18, "duration": 1.52}, {"text": "it will crash into a bunch\nof pedestrians crossing the street,", "start": 69.54, "duration": 4.12}, {"text": "but the car may swerve,", "start": 74.9, "duration": 2.135}, {"text": "hitting one bystander,", "start": 77.059, "duration": 1.857}, {"text": "killing them to save the pedestrians.", "start": 78.94, "duration": 2.08}, {"text": "What should the car do,\nand who should decide?", "start": 81.86, "duration": 2.6}, {"text": "What if instead the car\ncould swerve into a wall,", "start": 85.34, "duration": 3.536}, {"text": "crashing and killing you, the passenger,", "start": 88.9, "duration": 3.296}, {"text": "in order to save those pedestrians?", "start": 92.22, "duration": 2.32}, {"text": "This scenario is inspired\nby the trolley problem,", "start": 95.06, "duration": 3.08}, {"text": "which was invented\nby philosophers a few decades ago", "start": 98.78, "duration": 3.776}, {"text": "to think about ethics.", "start": 102.58, "duration": 1.24}, {"text": "Now, the way we think\nabout this problem matters.", "start": 105.94, "duration": 2.496}, {"text": "We may for example\nnot think about it at all.", "start": 108.46, "duration": 2.616}, {"text": "We may say this scenario is unrealistic,", "start": 111.1, "duration": 3.376}, {"text": "incredibly unlikely, or just silly.", "start": 114.5, "duration": 2.32}, {"text": "But I think this criticism\nmisses the point", "start": 117.58, "duration": 2.736}, {"text": "because it takes\nthe scenario too literally.", "start": 120.34, "duration": 2.16}, {"text": "Of course no accident\nis going to look like this;", "start": 123.74, "duration": 2.736}, {"text": "no accident has two or three options", "start": 126.5, "duration": 3.336}, {"text": "where everybody dies somehow.", "start": 129.86, "duration": 2.0}, {"text": "Instead, the car is going\nto calculate something", "start": 133.3, "duration": 2.576}, {"text": "like the probability of hitting\na certain group of people,", "start": 135.9, "duration": 4.896}, {"text": "if you swerve one direction\nversus another direction,", "start": 140.82, "duration": 3.336}, {"text": "you might slightly increase the risk\nto passengers or other drivers", "start": 144.18, "duration": 3.456}, {"text": "versus pedestrians.", "start": 147.66, "duration": 1.536}, {"text": "It's going to be\na more complex calculation,", "start": 149.22, "duration": 2.16}, {"text": "but it's still going\nto involve trade-offs,", "start": 152.3, "duration": 2.52}, {"text": "and trade-offs often require ethics.", "start": 155.66, "duration": 2.88}, {"text": "We might say then,\n\"Well, let's not worry about this.", "start": 159.66, "duration": 2.736}, {"text": "Let's wait until technology\nis fully ready and 100 percent safe.\"", "start": 162.42, "duration": 4.64}, {"text": "Suppose that we can indeed\neliminate 90 percent of those accidents,", "start": 168.34, "duration": 3.68}, {"text": "or even 99 percent in the next 10 years.", "start": 172.9, "duration": 2.84}, {"text": "What if eliminating\nthe last one percent of accidents", "start": 176.74, "duration": 3.176}, {"text": "requires 50 more years of research?", "start": 179.94, "duration": 3.12}, {"text": "Should we not adopt the technology?", "start": 184.22, "duration": 1.8}, {"text": "That's 60 million people\ndead in car accidents", "start": 186.54, "duration": 4.776}, {"text": "if we maintain the current rate.", "start": 191.34, "duration": 1.76}, {"text": "So the point is,", "start": 194.58, "duration": 1.216}, {"text": "waiting for full safety is also a choice,", "start": 195.82, "duration": 3.616}, {"text": "and it also involves trade-offs.", "start": 199.46, "duration": 2.16}, {"text": "People online on social media\nhave been coming up with all sorts of ways", "start": 203.38, "duration": 4.336}, {"text": "to not think about this problem.", "start": 207.74, "duration": 2.016}, {"text": "One person suggested\nthe car should just swerve somehow", "start": 209.78, "duration": 3.216}, {"text": "in between the passengers --", "start": 213.02, "duration": 2.136}, {"text": "(Laughter)", "start": 215.18, "duration": 1.016}, {"text": "and the bystander.", "start": 216.22, "duration": 1.256}, {"text": "Of course if that's what the car can do,\nthat's what the car should do.", "start": 217.5, "duration": 3.36}, {"text": "We're interested in scenarios\nin which this is not possible.", "start": 221.74, "duration": 2.84}, {"text": "And my personal favorite\nwas a suggestion by a blogger", "start": 225.1, "duration": 5.416}, {"text": "to have an eject button in the car\nthat you press --", "start": 230.54, "duration": 3.016}, {"text": "(Laughter)", "start": 233.58, "duration": 1.216}, {"text": "just before the car self-destructs.", "start": 234.82, "duration": 1.667}, {"text": "(Laughter)", "start": 236.511, "duration": 1.68}, {"text": "So if we acknowledge that cars\nwill have to make trade-offs on the road,", "start": 239.66, "duration": 5.2}, {"text": "how do we think about those trade-offs,", "start": 246.02, "duration": 1.88}, {"text": "and how do we decide?", "start": 249.14, "duration": 1.576}, {"text": "Well, maybe we should run a survey\nto find out what society wants,", "start": 250.74, "duration": 3.136}, {"text": "because ultimately,", "start": 253.9, "duration": 1.456}, {"text": "regulations and the law\nare a reflection of societal values.", "start": 255.38, "duration": 3.96}, {"text": "So this is what we did.", "start": 259.86, "duration": 1.24}, {"text": "With my collaborators,", "start": 261.7, "duration": 1.616}, {"text": "Jean-Fran\u00e7ois Bonnefon and Azim Shariff,", "start": 263.34, "duration": 2.336}, {"text": "we ran a survey", "start": 265.7, "duration": 1.616}, {"text": "in which we presented people\nwith these types of scenarios.", "start": 267.34, "duration": 2.855}, {"text": "We gave them two options\ninspired by two philosophers:", "start": 270.219, "duration": 3.777}, {"text": "Jeremy Bentham and Immanuel Kant.", "start": 274.02, "duration": 2.64}, {"text": "Bentham says the car\nshould follow utilitarian ethics:", "start": 277.42, "duration": 3.096}, {"text": "it should take the action\nthat will minimize total harm --", "start": 280.54, "duration": 3.416}, {"text": "even if that action will kill a bystander", "start": 283.98, "duration": 2.816}, {"text": "and even if that action\nwill kill the passenger.", "start": 286.82, "duration": 2.44}, {"text": "Immanuel Kant says the car\nshould follow duty-bound principles,", "start": 289.94, "duration": 4.976}, {"text": "like \"Thou shalt not kill.\"", "start": 294.94, "duration": 1.56}, {"text": "So you should not take an action\nthat explicitly harms a human being,", "start": 297.3, "duration": 4.456}, {"text": "and you should let the car take its course", "start": 301.78, "duration": 2.456}, {"text": "even if that's going to harm more people.", "start": 304.26, "duration": 1.96}, {"text": "What do you think?", "start": 307.46, "duration": 1.2}, {"text": "Bentham or Kant?", "start": 309.18, "duration": 1.52}, {"text": "Here's what we found.", "start": 311.58, "duration": 1.256}, {"text": "Most people sided with Bentham.", "start": 312.86, "duration": 1.8}, {"text": "So it seems that people\nwant cars to be utilitarian,", "start": 315.98, "duration": 3.776}, {"text": "minimize total harm,", "start": 319.78, "duration": 1.416}, {"text": "and that's what we should all do.", "start": 321.22, "duration": 1.576}, {"text": "Problem solved.", "start": 322.82, "duration": 1.2}, {"text": "But there is a little catch.", "start": 325.06, "duration": 1.48}, {"text": "When we asked people\nwhether they would purchase such cars,", "start": 327.74, "duration": 3.736}, {"text": "they said, \"Absolutely not.\"", "start": 331.5, "duration": 1.616}, {"text": "(Laughter)", "start": 333.14, "duration": 2.296}, {"text": "They would like to buy cars\nthat protect them at all costs,", "start": 335.46, "duration": 3.896}, {"text": "but they want everybody else\nto buy cars that minimize harm.", "start": 339.38, "duration": 3.616}, {"text": "(Laughter)", "start": 343.02, "duration": 2.52}, {"text": "We've seen this problem before.", "start": 346.54, "duration": 1.856}, {"text": "It's called a social dilemma.", "start": 348.42, "duration": 1.56}, {"text": "And to understand the social dilemma,", "start": 350.98, "duration": 1.816}, {"text": "we have to go a little bit\nback in history.", "start": 352.82, "duration": 2.04}, {"text": "In the 1800s,", "start": 355.82, "duration": 2.576}, {"text": "English economist William Forster Lloyd\npublished a pamphlet", "start": 358.42, "duration": 3.736}, {"text": "which describes the following scenario.", "start": 362.18, "duration": 2.216}, {"text": "You have a group of farmers --", "start": 364.42, "duration": 1.656}, {"text": "English farmers --", "start": 366.1, "duration": 1.336}, {"text": "who are sharing a common land\nfor their sheep to graze.", "start": 367.46, "duration": 2.68}, {"text": "Now, if each farmer\nbrings a certain number of sheep --", "start": 371.34, "duration": 2.576}, {"text": "let's say three sheep --", "start": 373.94, "duration": 1.496}, {"text": "the land will be rejuvenated,", "start": 375.46, "duration": 2.096}, {"text": "the farmers are happy,", "start": 377.58, "duration": 1.216}, {"text": "the sheep are happy,", "start": 378.82, "duration": 1.616}, {"text": "everything is good.", "start": 380.46, "duration": 1.2}, {"text": "Now, if one farmer brings one extra sheep,", "start": 382.26, "duration": 2.52}, {"text": "that farmer will do slightly better,\nand no one else will be harmed.", "start": 385.62, "duration": 4.72}, {"text": "But if every farmer made\nthat individually rational decision,", "start": 390.98, "duration": 3.64}, {"text": "the land will be overrun,\nand it will be depleted", "start": 395.66, "duration": 2.72}, {"text": "to the detriment of all the farmers,", "start": 399.18, "duration": 2.176}, {"text": "and of course,\nto the detriment of the sheep.", "start": 401.38, "duration": 2.12}, {"text": "We see this problem in many places:", "start": 404.54, "duration": 3.68}, {"text": "in the difficulty of managing overfishing,", "start": 408.9, "duration": 3.176}, {"text": "or in reducing carbon emissions\nto mitigate climate change.", "start": 412.1, "duration": 4.56}, {"text": "When it comes to the regulation\nof driverless cars,", "start": 418.98, "duration": 2.92}, {"text": "the common land now\nis basically public safety --", "start": 422.9, "duration": 4.336}, {"text": "that's the common good --", "start": 427.26, "duration": 1.24}, {"text": "and the farmers are the passengers", "start": 429.22, "duration": 1.976}, {"text": "or the car owners who are choosing\nto ride in those cars.", "start": 431.22, "duration": 3.6}, {"text": "And by making the individually\nrational choice", "start": 436.78, "duration": 2.616}, {"text": "of prioritizing their own safety,", "start": 439.42, "duration": 2.816}, {"text": "they may collectively be\ndiminishing the common good,", "start": 442.26, "duration": 3.136}, {"text": "which is minimizing total harm.", "start": 445.42, "duration": 2.2}, {"text": "It's called the tragedy of the commons,", "start": 450.14, "duration": 2.136}, {"text": "traditionally,", "start": 452.3, "duration": 1.296}, {"text": "but I think in the case\nof driverless cars,", "start": 453.62, "duration": 3.096}, {"text": "the problem may be\na little bit more insidious", "start": 456.74, "duration": 2.856}, {"text": "because there is not necessarily\nan individual human being", "start": 459.62, "duration": 3.496}, {"text": "making those decisions.", "start": 463.14, "duration": 1.696}, {"text": "So car manufacturers\nmay simply program cars", "start": 464.86, "duration": 3.296}, {"text": "that will maximize safety\nfor their clients,", "start": 468.18, "duration": 2.52}, {"text": "and those cars may learn\nautomatically on their own", "start": 471.9, "duration": 2.976}, {"text": "that doing so requires slightly\nincreasing risk for pedestrians.", "start": 474.9, "duration": 3.52}, {"text": "So to use the sheep metaphor,", "start": 479.34, "duration": 1.416}, {"text": "it's like we now have electric sheep\nthat have a mind of their own.", "start": 480.78, "duration": 3.616}, {"text": "(Laughter)", "start": 484.42, "duration": 1.456}, {"text": "And they may go and graze\neven if the farmer doesn't know it.", "start": 485.9, "duration": 3.08}, {"text": "So this is what we may call\nthe tragedy of the algorithmic commons,", "start": 490.46, "duration": 3.976}, {"text": "and if offers new types of challenges.", "start": 494.46, "duration": 2.36}, {"text": "Typically, traditionally,", "start": 502.34, "duration": 1.896}, {"text": "we solve these types\nof social dilemmas using regulation,", "start": 504.26, "duration": 3.336}, {"text": "so either governments\nor communities get together,", "start": 507.62, "duration": 2.736}, {"text": "and they decide collectively\nwhat kind of outcome they want", "start": 510.38, "duration": 3.736}, {"text": "and what sort of constraints\non individual behavior", "start": 514.14, "duration": 2.656}, {"text": "they need to implement.", "start": 516.82, "duration": 1.2}, {"text": "And then using monitoring and enforcement,", "start": 519.42, "duration": 2.616}, {"text": "they can make sure\nthat the public good is preserved.", "start": 522.06, "duration": 2.559}, {"text": "So why don't we just,", "start": 525.26, "duration": 1.575}, {"text": "as regulators,", "start": 526.859, "duration": 1.496}, {"text": "require that all cars minimize harm?", "start": 528.379, "duration": 2.897}, {"text": "After all, this is\nwhat people say they want.", "start": 531.3, "duration": 2.24}, {"text": "And more importantly,", "start": 535.02, "duration": 1.416}, {"text": "I can be sure that as an individual,", "start": 536.46, "duration": 3.096}, {"text": "if I buy a car that may\nsacrifice me in a very rare case,", "start": 539.58, "duration": 3.856}, {"text": "I'm not the only sucker doing that", "start": 543.46, "duration": 1.656}, {"text": "while everybody else\nenjoys unconditional protection.", "start": 545.14, "duration": 2.68}, {"text": "In our survey, we did ask people\nwhether they would support regulation", "start": 548.94, "duration": 3.336}, {"text": "and here's what we found.", "start": 552.3, "duration": 1.2}, {"text": "First of all, people\nsaid no to regulation;", "start": 554.18, "duration": 3.76}, {"text": "and second, they said,", "start": 559.1, "duration": 1.256}, {"text": "\"Well if you regulate cars to do this\nand to minimize total harm,", "start": 560.38, "duration": 3.936}, {"text": "I will not buy those cars.\"", "start": 564.34, "duration": 1.48}, {"text": "So ironically,", "start": 567.22, "duration": 1.376}, {"text": "by regulating cars to minimize harm,", "start": 568.62, "duration": 3.496}, {"text": "we may actually end up with more harm", "start": 572.14, "duration": 1.84}, {"text": "because people may not\nopt into the safer technology", "start": 574.86, "duration": 3.656}, {"text": "even if it's much safer\nthan human drivers.", "start": 578.54, "duration": 2.08}, {"text": "I don't have the final\nanswer to this riddle,", "start": 582.18, "duration": 3.416}, {"text": "but I think as a starting point,", "start": 585.62, "duration": 1.576}, {"text": "we need society to come together", "start": 587.22, "duration": 3.296}, {"text": "to decide what trade-offs\nwe are comfortable with", "start": 590.54, "duration": 2.76}, {"text": "and to come up with ways\nin which we can enforce those trade-offs.", "start": 594.18, "duration": 3.48}, {"text": "As a starting point,\nmy brilliant students,", "start": 598.34, "duration": 2.536}, {"text": "Edmond Awad and Sohan Dsouza,", "start": 600.9, "duration": 2.456}, {"text": "built the Moral Machine website,", "start": 603.38, "duration": 1.8}, {"text": "which generates random scenarios at you --", "start": 606.02, "duration": 2.68}, {"text": "basically a bunch\nof random dilemmas in a sequence", "start": 609.9, "duration": 2.456}, {"text": "where you have to choose what\nthe car should do in a given scenario.", "start": 612.38, "duration": 3.92}, {"text": "And we vary the ages and even\nthe species of the different victims.", "start": 616.86, "duration": 4.6}, {"text": "So far we've collected\nover five million decisions", "start": 622.86, "duration": 3.696}, {"text": "by over one million people worldwide", "start": 626.58, "duration": 2.2}, {"text": "from the website.", "start": 630.22, "duration": 1.2}, {"text": "And this is helping us\nform an early picture", "start": 632.18, "duration": 2.416}, {"text": "of what trade-offs\npeople are comfortable with", "start": 634.62, "duration": 2.616}, {"text": "and what matters to them --", "start": 637.26, "duration": 1.896}, {"text": "even across cultures.", "start": 639.18, "duration": 1.44}, {"text": "But more importantly,", "start": 642.06, "duration": 1.496}, {"text": "doing this exercise\nis helping people recognize", "start": 643.58, "duration": 3.376}, {"text": "the difficulty of making those choices", "start": 646.98, "duration": 2.816}, {"text": "and that the regulators\nare tasked with impossible choices.", "start": 649.82, "duration": 3.8}, {"text": "And maybe this will help us as a society\nunderstand the kinds of trade-offs", "start": 655.18, "duration": 3.576}, {"text": "that will be implemented\nultimately in regulation.", "start": 658.78, "duration": 3.056}, {"text": "And indeed, I was very happy to hear", "start": 661.86, "duration": 1.736}, {"text": "that the first set of regulations", "start": 663.62, "duration": 2.016}, {"text": "that came from\nthe Department of Transport --", "start": 665.66, "duration": 2.136}, {"text": "announced last week --", "start": 667.82, "duration": 1.376}, {"text": "included a 15-point checklist\nfor all carmakers to provide,", "start": 669.22, "duration": 6.576}, {"text": "and number 14 was ethical consideration --", "start": 675.82, "duration": 3.256}, {"text": "how are you going to deal with that.", "start": 679.1, "duration": 1.72}, {"text": "We also have people\nreflect on their own decisions", "start": 683.62, "duration": 2.656}, {"text": "by giving them summaries\nof what they chose.", "start": 686.3, "duration": 3.0}, {"text": "I'll give you one example --", "start": 690.26, "duration": 1.656}, {"text": "I'm just going to warn you\nthat this is not your typical example,", "start": 691.94, "duration": 3.536}, {"text": "your typical user.", "start": 695.5, "duration": 1.376}, {"text": "This is the most sacrificed and the most\nsaved character for this person.", "start": 696.9, "duration": 3.616}, {"text": "(Laughter)", "start": 700.54, "duration": 5.2}, {"text": "Some of you may agree with him,", "start": 706.5, "duration": 1.896}, {"text": "or her, we don't know.", "start": 708.42, "duration": 1.64}, {"text": "But this person also seems to slightly\nprefer passengers over pedestrians", "start": 712.3, "duration": 6.136}, {"text": "in their choices", "start": 718.46, "duration": 2.096}, {"text": "and is very happy to punish jaywalking.", "start": 720.58, "duration": 2.816}, {"text": "(Laughter)", "start": 723.42, "duration": 3.04}, {"text": "So let's wrap up.", "start": 729.14, "duration": 1.216}, {"text": "We started with the question --\nlet's call it the ethical dilemma --", "start": 730.379, "duration": 3.416}, {"text": "of what the car should do\nin a specific scenario:", "start": 733.82, "duration": 3.056}, {"text": "swerve or stay?", "start": 736.9, "duration": 1.2}, {"text": "But then we realized\nthat the problem was a different one.", "start": 739.06, "duration": 2.736}, {"text": "It was the problem of how to get\nsociety to agree on and enforce", "start": 741.82, "duration": 4.536}, {"text": "the trade-offs they're comfortable with.", "start": 746.38, "duration": 1.936}, {"text": "It's a social dilemma.", "start": 748.34, "duration": 1.256}, {"text": "In the 1940s, Isaac Asimov\nwrote his famous laws of robotics --", "start": 749.62, "duration": 5.016}, {"text": "the three laws of robotics.", "start": 754.66, "duration": 1.32}, {"text": "A robot may not harm a human being,", "start": 757.06, "duration": 2.456}, {"text": "a robot may not disobey a human being,", "start": 759.54, "duration": 2.536}, {"text": "and a robot may not allow\nitself to come to harm --", "start": 762.1, "duration": 3.256}, {"text": "in this order of importance.", "start": 765.38, "duration": 1.96}, {"text": "But after 40 years or so", "start": 768.18, "duration": 2.136}, {"text": "and after so many stories\npushing these laws to the limit,", "start": 770.34, "duration": 3.736}, {"text": "Asimov introduced the zeroth law", "start": 774.1, "duration": 3.696}, {"text": "which takes precedence above all,", "start": 777.82, "duration": 2.256}, {"text": "and it's that a robot\nmay not harm humanity as a whole.", "start": 780.1, "duration": 3.28}, {"text": "I don't know what this means\nin the context of driverless cars", "start": 784.3, "duration": 4.376}, {"text": "or any specific situation,", "start": 788.7, "duration": 2.736}, {"text": "and I don't know how we can implement it,", "start": 791.46, "duration": 2.216}, {"text": "but I think that by recognizing", "start": 793.7, "duration": 1.536}, {"text": "that the regulation of driverless cars\nis not only a technological problem", "start": 795.26, "duration": 6.136}, {"text": "but also a societal cooperation problem,", "start": 801.42, "duration": 3.28}, {"text": "I hope that we can at least begin\nto ask the right questions.", "start": 805.62, "duration": 2.88}, {"text": "Thank you.", "start": 809.02, "duration": 1.216}, {"text": "(Applause)", "start": 810.26, "duration": 2.92}]