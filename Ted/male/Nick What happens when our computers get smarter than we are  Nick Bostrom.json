[{"text": "I work with a bunch of mathematicians,\nphilosophers and computer scientists,", "start": 12.57, "duration": 4.207}, {"text": "and we sit around and think about\nthe future of machine intelligence,", "start": 16.777, "duration": 5.209}, {"text": "among other things.", "start": 21.986, "duration": 2.044}, {"text": "Some people think that some of these\nthings are sort of science fiction-y,", "start": 24.03, "duration": 4.725}, {"text": "far out there, crazy.", "start": 28.755, "duration": 3.101}, {"text": "But I like to say,", "start": 31.856, "duration": 1.47}, {"text": "okay, let's look at the modern\nhuman condition.", "start": 33.326, "duration": 3.604}, {"text": "(Laughter)", "start": 36.93, "duration": 1.692}, {"text": "This is the normal way for things to be.", "start": 38.622, "duration": 2.402}, {"text": "But if we think about it,", "start": 41.024, "duration": 2.285}, {"text": "we are actually recently arrived\nguests on this planet,", "start": 43.309, "duration": 3.293}, {"text": "the human species.", "start": 46.602, "duration": 2.082}, {"text": "Think about if Earth\nwas created one year ago,", "start": 48.684, "duration": 4.746}, {"text": "the human species, then, \nwould be 10 minutes old.", "start": 53.43, "duration": 3.548}, {"text": "The industrial era started\ntwo seconds ago.", "start": 56.978, "duration": 3.168}, {"text": "Another way to look at this is to think of\nworld GDP over the last 10,000 years,", "start": 61.276, "duration": 5.225}, {"text": "I've actually taken the trouble\nto plot this for you in a graph.", "start": 66.501, "duration": 3.029}, {"text": "It looks like this.", "start": 69.53, "duration": 1.774}, {"text": "(Laughter)", "start": 71.304, "duration": 1.363}, {"text": "It's a curious shape\nfor a normal condition.", "start": 72.667, "duration": 2.151}, {"text": "I sure wouldn't want to sit on it.", "start": 74.818, "duration": 1.698}, {"text": "(Laughter)", "start": 76.516, "duration": 2.551}, {"text": "Let's ask ourselves, what is the cause\nof this current anomaly?", "start": 79.067, "duration": 4.774}, {"text": "Some people would say it's technology.", "start": 83.841, "duration": 2.552}, {"text": "Now it's true, technology has accumulated\nthrough human history,", "start": 86.393, "duration": 4.668}, {"text": "and right now, technology\nadvances extremely rapidly --", "start": 91.061, "duration": 4.652}, {"text": "that is the proximate cause,", "start": 95.713, "duration": 1.565}, {"text": "that's why we are currently \nso very productive.", "start": 97.278, "duration": 2.565}, {"text": "But I like to think back further \nto the ultimate cause.", "start": 100.473, "duration": 3.661}, {"text": "Look at these two highly\ndistinguished gentlemen:", "start": 105.114, "duration": 3.766}, {"text": "We have Kanzi --", "start": 108.88, "duration": 1.6}, {"text": "he's mastered 200 lexical\ntokens, an incredible feat.", "start": 110.48, "duration": 4.643}, {"text": "And Ed Witten unleashed the second\nsuperstring revolution.", "start": 115.123, "duration": 3.694}, {"text": "If we look under the hood, \nthis is what we find:", "start": 118.817, "duration": 2.324}, {"text": "basically the same thing.", "start": 121.141, "duration": 1.57}, {"text": "One is a little larger,", "start": 122.711, "duration": 1.813}, {"text": "it maybe also has a few tricks\nin the exact way it's wired.", "start": 124.524, "duration": 2.758}, {"text": "These invisible differences cannot\nbe too complicated, however,", "start": 127.282, "duration": 3.812}, {"text": "because there have only\nbeen 250,000 generations", "start": 131.094, "duration": 4.285}, {"text": "since our last common ancestor.", "start": 135.379, "duration": 1.732}, {"text": "We know that complicated mechanisms\ntake a long time to evolve.", "start": 137.111, "duration": 3.849}, {"text": "So a bunch of relatively minor changes", "start": 142.0, "duration": 2.499}, {"text": "take us from Kanzi to Witten,", "start": 144.499, "duration": 3.067}, {"text": "from broken-off tree branches\nto intercontinental ballistic missiles.", "start": 147.566, "duration": 4.543}, {"text": "So this then seems pretty obvious\nthat everything we've achieved,", "start": 152.839, "duration": 3.935}, {"text": "and everything we care about,", "start": 156.774, "duration": 1.378}, {"text": "depends crucially on some relatively minor\nchanges that made the human mind.", "start": 158.152, "duration": 5.228}, {"text": "And the corollary, of course,\nis that any further changes", "start": 164.65, "duration": 3.662}, {"text": "that could significantly change\nthe substrate of thinking", "start": 168.312, "duration": 3.477}, {"text": "could have potentially \nenormous consequences.", "start": 171.789, "duration": 3.202}, {"text": "Some of my colleagues \nthink we're on the verge", "start": 176.321, "duration": 2.905}, {"text": "of something that could cause\na profound change in that substrate,", "start": 179.226, "duration": 3.908}, {"text": "and that is machine superintelligence.", "start": 183.134, "duration": 3.213}, {"text": "Artificial intelligence used to be\nabout putting commands in a box.", "start": 186.347, "duration": 4.739}, {"text": "You would have human programmers", "start": 191.086, "duration": 1.665}, {"text": "that would painstakingly \nhandcraft knowledge items.", "start": 192.751, "duration": 3.135}, {"text": "You build up these expert systems,", "start": 195.886, "duration": 2.086}, {"text": "and they were kind of useful \nfor some purposes,", "start": 197.972, "duration": 2.324}, {"text": "but they were very brittle,\nyou couldn't scale them.", "start": 200.296, "duration": 2.681}, {"text": "Basically, you got out only\nwhat you put in.", "start": 202.977, "duration": 3.433}, {"text": "But since then,", "start": 206.41, "duration": 0.997}, {"text": "a paradigm shift has taken place\nin the field of artificial intelligence.", "start": 207.407, "duration": 3.467}, {"text": "Today, the action is really \naround machine learning.", "start": 210.874, "duration": 2.77}, {"text": "So rather than handcrafting knowledge\nrepresentations and features,", "start": 214.394, "duration": 5.387}, {"text": "we create algorithms that learn,\noften from raw perceptual data.", "start": 220.511, "duration": 5.554}, {"text": "Basically the same thing\nthat the human infant does.", "start": 226.065, "duration": 4.998}, {"text": "The result is A.I. that is not\nlimited to one domain --", "start": 231.063, "duration": 4.207}, {"text": "the same system can learn to translate \nbetween any pairs of languages,", "start": 235.27, "duration": 4.631}, {"text": "or learn to play any computer game\non the Atari console.", "start": 239.901, "duration": 5.437}, {"text": "Now of course,", "start": 245.338, "duration": 1.779}, {"text": "A.I. is still nowhere near having\nthe same powerful, cross-domain", "start": 247.117, "duration": 3.999}, {"text": "ability to learn and plan\nas a human being has.", "start": 251.116, "duration": 3.219}, {"text": "The cortex still has some \nalgorithmic tricks", "start": 254.335, "duration": 2.126}, {"text": "that we don't yet know\nhow to match in machines.", "start": 256.461, "duration": 2.355}, {"text": "So the question is,", "start": 259.886, "duration": 1.899}, {"text": "how far are we from being able\nto match those tricks?", "start": 261.785, "duration": 3.5}, {"text": "A couple of years ago,", "start": 266.245, "duration": 1.083}, {"text": "we did a survey of some of the world's \nleading A.I. experts,", "start": 267.328, "duration": 2.888}, {"text": "to see what they think,\nand one of the questions we asked was,", "start": 270.216, "duration": 3.224}, {"text": "\"By which year do you think\nthere is a 50 percent probability", "start": 273.44, "duration": 3.353}, {"text": "that we will have achieved \nhuman-level machine intelligence?\"", "start": 276.793, "duration": 3.482}, {"text": "We defined human-level here \nas the ability to perform", "start": 280.785, "duration": 4.183}, {"text": "almost any job at least as well\nas an adult human,", "start": 284.968, "duration": 2.871}, {"text": "so real human-level, not just\nwithin some limited domain.", "start": 287.839, "duration": 4.005}, {"text": "And the median answer was 2040 or 2050,", "start": 291.844, "duration": 3.65}, {"text": "depending on precisely which \ngroup of experts we asked.", "start": 295.494, "duration": 2.806}, {"text": "Now, it could happen much,\nmuch later, or sooner,", "start": 298.3, "duration": 4.039}, {"text": "the truth is nobody really knows.", "start": 302.339, "duration": 1.94}, {"text": "What we do know is that the ultimate \nlimit to information processing", "start": 305.259, "duration": 4.412}, {"text": "in a machine substrate lies far outside \nthe limits in biological tissue.", "start": 309.671, "duration": 4.871}, {"text": "This comes down to physics.", "start": 315.241, "duration": 2.378}, {"text": "A biological neuron fires, maybe, \nat 200 hertz, 200 times a second.", "start": 317.619, "duration": 4.718}, {"text": "But even a present-day transistor\noperates at the Gigahertz.", "start": 322.337, "duration": 3.594}, {"text": "Neurons propagate slowly in axons,\n100 meters per second, tops.", "start": 325.931, "duration": 5.297}, {"text": "But in computers, signals can travel\nat the speed of light.", "start": 331.228, "duration": 3.111}, {"text": "There are also size limitations,", "start": 335.079, "duration": 1.869}, {"text": "like a human brain has \nto fit inside a cranium,", "start": 336.948, "duration": 3.027}, {"text": "but a computer can be the size\nof a warehouse or larger.", "start": 339.975, "duration": 4.761}, {"text": "So the potential for superintelligence \nlies dormant in matter,", "start": 344.736, "duration": 5.599}, {"text": "much like the power of the atom \nlay dormant throughout human history,", "start": 350.335, "duration": 5.712}, {"text": "patiently waiting there until 1945.", "start": 356.047, "duration": 4.405}, {"text": "In this century,", "start": 360.452, "duration": 1.248}, {"text": "scientists may learn to awaken\nthe power of artificial intelligence.", "start": 361.7, "duration": 4.118}, {"text": "And I think we might then see\nan intelligence explosion.", "start": 365.818, "duration": 4.008}, {"text": "Now most people, when they think\nabout what is smart and what is dumb,", "start": 370.406, "duration": 3.957}, {"text": "I think have in mind a picture\nroughly like this.", "start": 374.363, "duration": 3.023}, {"text": "So at one end we have the village idiot,", "start": 377.386, "duration": 2.598}, {"text": "and then far over at the other side", "start": 379.984, "duration": 2.483}, {"text": "we have Ed Witten, or Albert Einstein,\nor whoever your favorite guru is.", "start": 382.467, "duration": 4.756}, {"text": "But I think that from the point of view\nof artificial intelligence,", "start": 387.223, "duration": 3.834}, {"text": "the true picture is actually\nprobably more like this:", "start": 391.057, "duration": 3.681}, {"text": "AI starts out at this point here,\nat zero intelligence,", "start": 395.258, "duration": 3.378}, {"text": "and then, after many, many \nyears of really hard work,", "start": 398.636, "duration": 3.011}, {"text": "maybe eventually we get to\nmouse-level artificial intelligence,", "start": 401.647, "duration": 3.844}, {"text": "something that can navigate \ncluttered environments", "start": 405.491, "duration": 2.43}, {"text": "as well as a mouse can.", "start": 407.921, "duration": 1.987}, {"text": "And then, after many, many more years\nof really hard work, lots of investment,", "start": 409.908, "duration": 4.313}, {"text": "maybe eventually we get to\nchimpanzee-level artificial intelligence.", "start": 414.221, "duration": 4.639}, {"text": "And then, after even more years \nof really, really hard work,", "start": 418.86, "duration": 3.21}, {"text": "we get to village idiot \nartificial intelligence.", "start": 422.07, "duration": 2.913}, {"text": "And a few moments later, \nwe are beyond Ed Witten.", "start": 424.983, "duration": 3.272}, {"text": "The train doesn't stop\nat Humanville Station.", "start": 428.255, "duration": 2.97}, {"text": "It's likely, rather, to swoosh right by.", "start": 431.225, "duration": 3.022}, {"text": "Now this has profound implications,", "start": 434.247, "duration": 1.984}, {"text": "particularly when it comes \nto questions of power.", "start": 436.231, "duration": 3.862}, {"text": "For example, chimpanzees are strong --", "start": 440.093, "duration": 1.899}, {"text": "pound for pound, a chimpanzee is about\ntwice as strong as a fit human male.", "start": 441.992, "duration": 5.222}, {"text": "And yet, the fate of Kanzi \nand his pals depends a lot more", "start": 447.214, "duration": 4.614}, {"text": "on what we humans do than on\nwhat the chimpanzees do themselves.", "start": 451.828, "duration": 4.14}, {"text": "Once there is superintelligence,", "start": 457.228, "duration": 2.314}, {"text": "the fate of humanity may depend\non what the superintelligence does.", "start": 459.542, "duration": 3.839}, {"text": "Think about it:", "start": 464.451, "duration": 1.057}, {"text": "Machine intelligence is the last invention\nthat humanity will ever need to make.", "start": 465.508, "duration": 5.044}, {"text": "Machines will then be better \nat inventing than we are,", "start": 470.552, "duration": 2.973}, {"text": "and they'll be doing so \non digital timescales.", "start": 473.525, "duration": 2.54}, {"text": "What this means is basically\na telescoping of the future.", "start": 476.065, "duration": 4.901}, {"text": "Think of all the crazy technologies \nthat you could have imagined", "start": 480.966, "duration": 3.558}, {"text": "maybe humans could have developed\nin the fullness of time:", "start": 484.524, "duration": 2.798}, {"text": "cures for aging, space colonization,", "start": 487.322, "duration": 3.258}, {"text": "self-replicating nanobots or uploading\nof minds into computers,", "start": 490.58, "duration": 3.731}, {"text": "all kinds of science fiction-y stuff", "start": 494.311, "duration": 2.159}, {"text": "that's nevertheless consistent \nwith the laws of physics.", "start": 496.47, "duration": 2.737}, {"text": "All of this superintelligence could \ndevelop, and possibly quite rapidly.", "start": 499.207, "duration": 4.212}, {"text": "Now, a superintelligence with such \ntechnological maturity", "start": 504.449, "duration": 3.558}, {"text": "would be extremely powerful,", "start": 508.007, "duration": 2.179}, {"text": "and at least in some scenarios,\nit would be able to get what it wants.", "start": 510.186, "duration": 4.546}, {"text": "We would then have a future that would\nbe shaped by the preferences of this A.I.", "start": 514.732, "duration": 5.661}, {"text": "Now a good question is,\nwhat are those preferences?", "start": 521.855, "duration": 3.749}, {"text": "Here it gets trickier.", "start": 526.244, "duration": 1.769}, {"text": "To make any headway with this,", "start": 528.013, "duration": 1.435}, {"text": "we must first of all\navoid anthropomorphizing.", "start": 529.448, "duration": 3.276}, {"text": "And this is ironic because \nevery newspaper article", "start": 533.934, "duration": 3.301}, {"text": "about the future of A.I.\nhas a picture of this:", "start": 537.235, "duration": 3.855}, {"text": "So I think what we need to do is\nto conceive of the issue more abstractly,", "start": 542.28, "duration": 4.134}, {"text": "not in terms of vivid Hollywood scenarios.", "start": 546.414, "duration": 2.79}, {"text": "We need to think of intelligence \nas an optimization process,", "start": 549.204, "duration": 3.617}, {"text": "a process that steers the future\ninto a particular set of configurations.", "start": 552.821, "duration": 5.649}, {"text": "A superintelligence is\na really strong optimization process.", "start": 558.47, "duration": 3.511}, {"text": "It's extremely good at using \navailable means to achieve a state", "start": 561.981, "duration": 4.117}, {"text": "in which its goal is realized.", "start": 566.098, "duration": 1.909}, {"text": "This means that there is no necessary\nconnection between", "start": 568.447, "duration": 2.672}, {"text": "being highly intelligent in this sense,", "start": 571.119, "duration": 2.734}, {"text": "and having an objective that we humans\nwould find worthwhile or meaningful.", "start": 573.853, "duration": 4.662}, {"text": "Suppose we give an A.I. the goal \nto make humans smile.", "start": 579.321, "duration": 3.794}, {"text": "When the A.I. is weak, it performs useful\nor amusing actions", "start": 583.115, "duration": 2.982}, {"text": "that cause its user to smile.", "start": 586.097, "duration": 2.517}, {"text": "When the A.I. becomes superintelligent,", "start": 588.614, "duration": 2.417}, {"text": "it realizes that there is a more\neffective way to achieve this goal:", "start": 591.031, "duration": 3.523}, {"text": "take control of the world", "start": 594.554, "duration": 1.922}, {"text": "and stick electrodes into the facial\nmuscles of humans", "start": 596.476, "duration": 3.162}, {"text": "to cause constant, beaming grins.", "start": 599.638, "duration": 2.941}, {"text": "Another example,", "start": 602.579, "duration": 1.035}, {"text": "suppose we give A.I. the goal to solve\na difficult mathematical problem.", "start": 603.614, "duration": 3.383}, {"text": "When the A.I. becomes superintelligent,", "start": 606.997, "duration": 1.937}, {"text": "it realizes that the most effective way \nto get the solution to this problem", "start": 608.934, "duration": 4.171}, {"text": "is by transforming the planet\ninto a giant computer,", "start": 613.105, "duration": 2.93}, {"text": "so as to increase its thinking capacity.", "start": 616.035, "duration": 2.246}, {"text": "And notice that this gives the A.I.s\nan instrumental reason", "start": 618.281, "duration": 2.764}, {"text": "to do things to us that we\nmight not approve of.", "start": 621.045, "duration": 2.516}, {"text": "Human beings in this model are threats,", "start": 623.561, "duration": 1.935}, {"text": "we could prevent the mathematical\nproblem from being solved.", "start": 625.496, "duration": 2.921}, {"text": "Of course, perceivably things won't \ngo wrong in these particular ways;", "start": 629.207, "duration": 3.494}, {"text": "these are cartoon examples.", "start": 632.701, "duration": 1.753}, {"text": "But the general point here is important:", "start": 634.454, "duration": 1.939}, {"text": "if you create a really powerful\noptimization process", "start": 636.393, "duration": 2.873}, {"text": "to maximize for objective x,", "start": 639.266, "duration": 2.234}, {"text": "you better make sure \nthat your definition of x", "start": 641.5, "duration": 2.276}, {"text": "incorporates everything you care about.", "start": 643.776, "duration": 2.469}, {"text": "This is a lesson that's also taught\nin many a myth.", "start": 646.835, "duration": 4.384}, {"text": "King Midas wishes that everything\nhe touches be turned into gold.", "start": 651.219, "duration": 5.298}, {"text": "He touches his daughter, \nshe turns into gold.", "start": 656.517, "duration": 2.861}, {"text": "He touches his food, it turns into gold.", "start": 659.378, "duration": 2.553}, {"text": "This could become practically relevant,", "start": 661.931, "duration": 2.589}, {"text": "not just as a metaphor for greed,", "start": 664.52, "duration": 2.07}, {"text": "but as an illustration of what happens", "start": 666.59, "duration": 1.895}, {"text": "if you create a powerful\noptimization process", "start": 668.485, "duration": 2.837}, {"text": "and give it misconceived \nor poorly specified goals.", "start": 671.322, "duration": 4.789}, {"text": "Now you might say, if a computer starts\nsticking electrodes into people's faces,", "start": 676.111, "duration": 5.189}, {"text": "we'd just shut it off.", "start": 681.3, "duration": 2.265}, {"text": "A, this is not necessarily so easy to do\nif we've grown dependent on the system --", "start": 684.555, "duration": 5.34}, {"text": "like, where is the off switch \nto the Internet?", "start": 689.895, "duration": 2.732}, {"text": "B, why haven't the chimpanzees\nflicked the off switch to humanity,", "start": 692.627, "duration": 5.12}, {"text": "or the Neanderthals?", "start": 697.747, "duration": 1.551}, {"text": "They certainly had reasons.", "start": 699.298, "duration": 2.666}, {"text": "We have an off switch, \nfor example, right here.", "start": 701.964, "duration": 2.795}, {"text": "(Choking)", "start": 704.759, "duration": 1.554}, {"text": "The reason is that we are \nan intelligent adversary;", "start": 706.313, "duration": 2.925}, {"text": "we can anticipate threats \nand plan around them.", "start": 709.238, "duration": 2.728}, {"text": "But so could a superintelligent agent,", "start": 711.966, "duration": 2.504}, {"text": "and it would be much better \nat that than we are.", "start": 714.47, "duration": 3.254}, {"text": "The point is, we should not be confident\nthat we have this under control here.", "start": 717.724, "duration": 7.187}, {"text": "And we could try to make our job\na little bit easier by, say,", "start": 724.911, "duration": 3.447}, {"text": "putting the A.I. in a box,", "start": 728.358, "duration": 1.59}, {"text": "like a secure software environment,", "start": 729.948, "duration": 1.796}, {"text": "a virtual reality simulation\nfrom which it cannot escape.", "start": 731.744, "duration": 3.022}, {"text": "But how confident can we be that\nthe A.I. couldn't find a bug.", "start": 734.766, "duration": 4.146}, {"text": "Given that merely human hackers\nfind bugs all the time,", "start": 738.912, "duration": 3.169}, {"text": "I'd say, probably not very confident.", "start": 742.081, "duration": 3.036}, {"text": "So we disconnect the ethernet cable\nto create an air gap,", "start": 746.237, "duration": 4.548}, {"text": "but again, like merely human hackers", "start": 750.785, "duration": 2.668}, {"text": "routinely transgress air gaps\nusing social engineering.", "start": 753.453, "duration": 3.381}, {"text": "Right now, as I speak,", "start": 756.834, "duration": 1.259}, {"text": "I'm sure there is some employee\nout there somewhere", "start": 758.093, "duration": 2.389}, {"text": "who has been talked into handing out \nher account details", "start": 760.482, "duration": 3.346}, {"text": "by somebody claiming to be\nfrom the I.T. department.", "start": 763.828, "duration": 2.746}, {"text": "More creative scenarios are also possible,", "start": 766.574, "duration": 2.127}, {"text": "like if you're the A.I.,", "start": 768.701, "duration": 1.315}, {"text": "you can imagine wiggling electrodes\naround in your internal circuitry", "start": 770.016, "duration": 3.532}, {"text": "to create radio waves that you\ncan use to communicate.", "start": 773.548, "duration": 3.462}, {"text": "Or maybe you could pretend to malfunction,", "start": 777.01, "duration": 2.424}, {"text": "and then when the programmers open\nyou up to see what went wrong with you,", "start": 779.434, "duration": 3.497}, {"text": "they look at the source code -- Bam! --", "start": 782.931, "duration": 1.936}, {"text": "the manipulation can take place.", "start": 784.867, "duration": 2.447}, {"text": "Or it could output the blueprint\nto a really nifty technology,", "start": 787.314, "duration": 3.43}, {"text": "and when we implement it,", "start": 790.744, "duration": 1.398}, {"text": "it has some surreptitious side effect\nthat the A.I. had planned.", "start": 792.142, "duration": 4.397}, {"text": "The point here is that we should \nnot be confident in our ability", "start": 796.539, "duration": 3.463}, {"text": "to keep a superintelligent genie\nlocked up in its bottle forever.", "start": 800.002, "duration": 3.808}, {"text": "Sooner or later, it will out.", "start": 803.81, "duration": 2.254}, {"text": "I believe that the answer here\nis to figure out", "start": 807.034, "duration": 3.103}, {"text": "how to create superintelligent A.I.\nsuch that even if -- when -- it escapes,", "start": 810.137, "duration": 5.024}, {"text": "it is still safe because it is\nfundamentally on our side", "start": 815.161, "duration": 3.277}, {"text": "because it shares our values.", "start": 818.438, "duration": 1.899}, {"text": "I see no way around \nthis difficult problem.", "start": 820.337, "duration": 3.21}, {"text": "Now, I'm actually fairly optimistic\nthat this problem can be solved.", "start": 824.557, "duration": 3.834}, {"text": "We wouldn't have to write down \na long list of everything we care about,", "start": 828.391, "duration": 3.903}, {"text": "or worse yet, spell it out \nin some computer language", "start": 832.294, "duration": 3.643}, {"text": "like C++ or Python,", "start": 835.937, "duration": 1.454}, {"text": "that would be a task beyond hopeless.", "start": 837.391, "duration": 2.767}, {"text": "Instead, we would create an A.I.\nthat uses its intelligence", "start": 840.158, "duration": 4.297}, {"text": "to learn what we value,", "start": 844.455, "duration": 2.771}, {"text": "and its motivation system is constructed\nin such a way that it is motivated", "start": 847.226, "duration": 5.28}, {"text": "to pursue our values or to perform actions\nthat it predicts we would approve of.", "start": 852.506, "duration": 5.232}, {"text": "We would thus leverage \nits intelligence as much as possible", "start": 857.738, "duration": 3.414}, {"text": "to solve the problem of value-loading.", "start": 861.152, "duration": 2.745}, {"text": "This can happen,", "start": 864.727, "duration": 1.512}, {"text": "and the outcome could be \nvery good for humanity.", "start": 866.239, "duration": 3.596}, {"text": "But it doesn't happen automatically.", "start": 869.835, "duration": 3.957}, {"text": "The initial conditions \nfor the intelligence explosion", "start": 873.792, "duration": 2.998}, {"text": "might need to be set up \nin just the right way", "start": 876.79, "duration": 2.863}, {"text": "if we are to have a controlled detonation.", "start": 879.653, "duration": 3.53}, {"text": "The values that the A.I. has\nneed to match ours,", "start": 883.183, "duration": 2.618}, {"text": "not just in the familiar context,", "start": 885.801, "duration": 1.76}, {"text": "like where we can easily check\nhow the A.I. behaves,", "start": 887.561, "duration": 2.438}, {"text": "but also in all novel contexts\nthat the A.I. might encounter", "start": 889.999, "duration": 3.234}, {"text": "in the indefinite future.", "start": 893.233, "duration": 1.557}, {"text": "And there are also some esoteric issues\nthat would need to be solved, sorted out:", "start": 894.79, "duration": 4.737}, {"text": "the exact details of its decision theory,", "start": 899.527, "duration": 2.089}, {"text": "how to deal with logical\nuncertainty and so forth.", "start": 901.616, "duration": 2.864}, {"text": "So the technical problems that need\nto be solved to make this work", "start": 905.33, "duration": 3.102}, {"text": "look quite difficult --", "start": 908.432, "duration": 1.113}, {"text": "not as difficult as making \na superintelligent A.I.,", "start": 909.545, "duration": 3.38}, {"text": "but fairly difficult.", "start": 912.925, "duration": 2.868}, {"text": "Here is the worry:", "start": 915.793, "duration": 1.695}, {"text": "Making superintelligent A.I.\nis a really hard challenge.", "start": 917.488, "duration": 4.684}, {"text": "Making superintelligent A.I. that is safe", "start": 922.172, "duration": 2.548}, {"text": "involves some additional \nchallenge on top of that.", "start": 924.72, "duration": 2.416}, {"text": "The risk is that if somebody figures out\nhow to crack the first challenge", "start": 928.216, "duration": 3.487}, {"text": "without also having cracked \nthe additional challenge", "start": 931.703, "duration": 3.001}, {"text": "of ensuring perfect safety.", "start": 934.704, "duration": 1.901}, {"text": "So I think that we should\nwork out a solution", "start": 937.375, "duration": 3.331}, {"text": "to the control problem in advance,", "start": 940.706, "duration": 2.822}, {"text": "so that we have it available \nby the time it is needed.", "start": 943.528, "duration": 2.66}, {"text": "Now it might be that we cannot solve\nthe entire control problem in advance", "start": 946.768, "duration": 3.507}, {"text": "because maybe some elements\ncan only be put in place", "start": 950.275, "duration": 3.024}, {"text": "once you know the details of the \narchitecture where it will be implemented.", "start": 953.299, "duration": 3.997}, {"text": "But the more of the control problem\nthat we solve in advance,", "start": 957.296, "duration": 3.38}, {"text": "the better the odds that the transition\nto the machine intelligence era", "start": 960.676, "duration": 4.09}, {"text": "will go well.", "start": 964.766, "duration": 1.54}, {"text": "This to me looks like a thing\nthat is well worth doing", "start": 966.306, "duration": 4.644}, {"text": "and I can imagine that if \nthings turn out okay,", "start": 970.95, "duration": 3.332}, {"text": "that people a million years from now\nlook back at this century", "start": 974.282, "duration": 4.658}, {"text": "and it might well be that they say that\nthe one thing we did that really mattered", "start": 978.94, "duration": 4.002}, {"text": "was to get this thing right.", "start": 982.942, "duration": 1.567}, {"text": "Thank you.", "start": 984.509, "duration": 1.689}, {"text": "(Applause)", "start": 986.198, "duration": 2.813}]