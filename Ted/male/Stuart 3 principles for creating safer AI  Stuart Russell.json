[{"text": "This is Lee Sedol.", "start": 12.532, "duration": 1.552}, {"text": "Lee Sedol is one of the world's\ngreatest Go players,", "start": 14.108, "duration": 3.997}, {"text": "and he's having what my friends\nin Silicon Valley call", "start": 18.129, "duration": 2.885}, {"text": "a \"Holy Cow\" moment --", "start": 21.038, "duration": 1.51}, {"text": "(Laughter)", "start": 22.572, "duration": 1.073}, {"text": "a moment where we realize", "start": 23.669, "duration": 2.188}, {"text": "that AI is actually progressing\na lot faster than we expected.", "start": 25.881, "duration": 3.296}, {"text": "So humans have lost on the Go board.\nWhat about the real world?", "start": 29.974, "duration": 3.047}, {"text": "Well, the real world is much bigger,", "start": 33.045, "duration": 2.1}, {"text": "much more complicated than the Go board.", "start": 35.169, "duration": 2.249}, {"text": "It's a lot less visible,", "start": 37.442, "duration": 1.819}, {"text": "but it's still a decision problem.", "start": 39.285, "duration": 2.038}, {"text": "And if we think about some\nof the technologies", "start": 42.768, "duration": 2.321}, {"text": "that are coming down the pike ...", "start": 45.113, "duration": 1.749}, {"text": "Noriko [Arai] mentioned that reading\nis not yet happening in machines,", "start": 47.558, "duration": 4.335}, {"text": "at least with understanding.", "start": 51.917, "duration": 1.5}, {"text": "But that will happen,", "start": 53.441, "duration": 1.536}, {"text": "and when that happens,", "start": 55.001, "duration": 1.771}, {"text": "very soon afterwards,", "start": 56.796, "duration": 1.187}, {"text": "machines will have read everything\nthat the human race has ever written.", "start": 58.007, "duration": 4.572}, {"text": "And that will enable machines,", "start": 63.67, "duration": 2.03}, {"text": "along with the ability to look\nfurther ahead than humans can,", "start": 65.724, "duration": 2.92}, {"text": "as we've already seen in Go,", "start": 68.668, "duration": 1.68}, {"text": "if they also have access\nto more information,", "start": 70.372, "duration": 2.164}, {"text": "they'll be able to make better decisions\nin the real world than we can.", "start": 72.56, "duration": 4.268}, {"text": "So is that a good thing?", "start": 78.612, "duration": 1.606}, {"text": "Well, I hope so.", "start": 81.718, "duration": 2.232}, {"text": "Our entire civilization,\neverything that we value,", "start": 86.514, "duration": 3.255}, {"text": "is based on our intelligence.", "start": 89.793, "duration": 2.068}, {"text": "And if we had access\nto a lot more intelligence,", "start": 91.885, "duration": 3.694}, {"text": "then there's really no limit\nto what the human race can do.", "start": 95.603, "duration": 3.302}, {"text": "And I think this could be,\nas some people have described it,", "start": 100.485, "duration": 3.325}, {"text": "the biggest event in human history.", "start": 103.834, "duration": 2.016}, {"text": "So why are people saying things like this,", "start": 108.485, "duration": 2.829}, {"text": "that AI might spell the end\nof the human race?", "start": 111.338, "duration": 2.876}, {"text": "Is this a new thing?", "start": 115.258, "duration": 1.659}, {"text": "Is it just Elon Musk and Bill Gates\nand Stephen Hawking?", "start": 116.941, "duration": 4.11}, {"text": "Actually, no. This idea\nhas been around for a while.", "start": 121.773, "duration": 3.262}, {"text": "Here's a quotation:", "start": 125.059, "duration": 1.962}, {"text": "\"Even if we could keep the machines\nin a subservient position,", "start": 127.045, "duration": 4.35}, {"text": "for instance, by turning off the power\nat strategic moments\" --", "start": 131.419, "duration": 2.984}, {"text": "and I'll come back to that\n\"turning off the power\" idea later on --", "start": 134.427, "duration": 3.237}, {"text": "\"we should, as a species,\nfeel greatly humbled.\"", "start": 137.688, "duration": 2.804}, {"text": "So who said this?\nThis is Alan Turing in 1951.", "start": 141.997, "duration": 3.448}, {"text": "Alan Turing, as you know,\nis the father of computer science", "start": 146.12, "duration": 2.763}, {"text": "and in many ways,\nthe father of AI as well.", "start": 148.907, "duration": 3.048}, {"text": "So if we think about this problem,", "start": 153.059, "duration": 1.882}, {"text": "the problem of creating something\nmore intelligent than your own species,", "start": 154.965, "duration": 3.787}, {"text": "we might call this \"the gorilla problem,\"", "start": 158.776, "duration": 2.622}, {"text": "because gorillas' ancestors did this\na few million years ago,", "start": 162.165, "duration": 3.75}, {"text": "and now we can ask the gorillas:", "start": 165.939, "duration": 1.745}, {"text": "Was this a good idea?", "start": 168.572, "duration": 1.16}, {"text": "So here they are having a meeting\nto discuss whether it was a good idea,", "start": 169.756, "duration": 3.53}, {"text": "and after a little while,\nthey conclude, no,", "start": 173.31, "duration": 3.346}, {"text": "this was a terrible idea.", "start": 176.68, "duration": 1.345}, {"text": "Our species is in dire straits.", "start": 178.049, "duration": 1.782}, {"text": "In fact, you can see the existential\nsadness in their eyes.", "start": 180.358, "duration": 4.263}, {"text": "(Laughter)", "start": 184.645, "duration": 1.64}, {"text": "So this queasy feeling that making\nsomething smarter than your own species", "start": 186.309, "duration": 4.84}, {"text": "is maybe not a good idea --", "start": 191.173, "duration": 2.365}, {"text": "what can we do about that?", "start": 194.308, "duration": 1.491}, {"text": "Well, really nothing,\nexcept stop doing AI,", "start": 195.823, "duration": 4.767}, {"text": "and because of all\nthe benefits that I mentioned", "start": 200.614, "duration": 2.51}, {"text": "and because I'm an AI researcher,", "start": 203.148, "duration": 1.716}, {"text": "I'm not having that.", "start": 204.888, "duration": 1.791}, {"text": "I actually want to be able\nto keep doing AI.", "start": 207.103, "duration": 2.468}, {"text": "So we actually need to nail down\nthe problem a bit more.", "start": 210.435, "duration": 2.678}, {"text": "What exactly is the problem?", "start": 213.137, "duration": 1.371}, {"text": "Why is better AI possibly a catastrophe?", "start": 214.532, "duration": 3.246}, {"text": "So here's another quotation:", "start": 219.218, "duration": 1.498}, {"text": "\"We had better be quite sure\nthat the purpose put into the machine", "start": 221.755, "duration": 3.335}, {"text": "is the purpose which we really desire.\"", "start": 225.114, "duration": 2.298}, {"text": "This was said by Norbert Wiener in 1960,", "start": 228.102, "duration": 3.498}, {"text": "shortly after he watched\none of the very early learning systems", "start": 231.624, "duration": 4.002}, {"text": "learn to play checkers\nbetter than its creator.", "start": 235.65, "duration": 2.583}, {"text": "But this could equally have been said", "start": 240.422, "duration": 2.683}, {"text": "by King Midas.", "start": 243.129, "duration": 1.167}, {"text": "King Midas said, \"I want everything\nI touch to turn to gold,\"", "start": 244.903, "duration": 3.134}, {"text": "and he got exactly what he asked for.", "start": 248.061, "duration": 2.473}, {"text": "That was the purpose\nthat he put into the machine,", "start": 250.558, "duration": 2.751}, {"text": "so to speak,", "start": 253.333, "duration": 1.45}, {"text": "and then his food and his drink\nand his relatives turned to gold", "start": 254.807, "duration": 3.444}, {"text": "and he died in misery and starvation.", "start": 258.275, "duration": 2.281}, {"text": "So we'll call this\n\"the King Midas problem\"", "start": 262.264, "duration": 2.341}, {"text": "of stating an objective\nwhich is not, in fact,", "start": 264.629, "duration": 3.305}, {"text": "truly aligned with what we want.", "start": 267.958, "duration": 2.413}, {"text": "In modern terms, we call this\n\"the value alignment problem.\"", "start": 270.395, "duration": 3.253}, {"text": "Putting in the wrong objective\nis not the only part of the problem.", "start": 276.867, "duration": 3.485}, {"text": "There's another part.", "start": 280.376, "duration": 1.152}, {"text": "If you put an objective into a machine,", "start": 281.98, "duration": 1.943}, {"text": "even something as simple as,\n\"Fetch the coffee,\"", "start": 283.947, "duration": 2.448}, {"text": "the machine says to itself,", "start": 287.728, "duration": 1.841}, {"text": "\"Well, how might I fail\nto fetch the coffee?", "start": 290.553, "duration": 2.623}, {"text": "Someone might switch me off.", "start": 293.2, "duration": 1.58}, {"text": "OK, I have to take steps to prevent that.", "start": 295.465, "duration": 2.387}, {"text": "I will disable my 'off' switch.", "start": 297.876, "duration": 1.906}, {"text": "I will do anything to defend myself\nagainst interference", "start": 300.354, "duration": 2.959}, {"text": "with this objective\nthat I have been given.\"", "start": 303.337, "duration": 2.629}, {"text": "So this single-minded pursuit", "start": 305.99, "duration": 2.012}, {"text": "in a very defensive mode\nof an objective that is, in fact,", "start": 309.033, "duration": 2.945}, {"text": "not aligned with the true objectives\nof the human race --", "start": 312.002, "duration": 2.814}, {"text": "that's the problem that we face.", "start": 315.942, "duration": 1.862}, {"text": "And in fact, that's the high-value\ntakeaway from this talk.", "start": 318.827, "duration": 4.767}, {"text": "If you want to remember one thing,", "start": 323.618, "duration": 2.055}, {"text": "it's that you can't fetch\nthe coffee if you're dead.", "start": 325.697, "duration": 2.675}, {"text": "(Laughter)", "start": 328.396, "duration": 1.061}, {"text": "It's very simple. Just remember that.\nRepeat it to yourself three times a day.", "start": 329.481, "duration": 3.829}, {"text": "(Laughter)", "start": 333.334, "duration": 1.821}, {"text": "And in fact, this is exactly the plot", "start": 335.179, "duration": 2.754}, {"text": "of \"2001: [A Space Odyssey]\"", "start": 337.957, "duration": 2.648}, {"text": "HAL has an objective, a mission,", "start": 341.046, "duration": 2.09}, {"text": "which is not aligned\nwith the objectives of the humans,", "start": 343.16, "duration": 3.732}, {"text": "and that leads to this conflict.", "start": 346.916, "duration": 1.81}, {"text": "Now fortunately, HAL\nis not superintelligent.", "start": 349.314, "duration": 2.969}, {"text": "He's pretty smart,\nbut eventually Dave outwits him", "start": 352.307, "duration": 3.587}, {"text": "and manages to switch him off.", "start": 355.918, "duration": 1.849}, {"text": "But we might not be so lucky.", "start": 361.648, "duration": 1.619}, {"text": "So what are we going to do?", "start": 368.013, "duration": 1.592}, {"text": "I'm trying to redefine AI", "start": 372.191, "duration": 2.601}, {"text": "to get away from this classical notion", "start": 374.816, "duration": 2.061}, {"text": "of machines that intelligently\npursue objectives.", "start": 376.901, "duration": 4.567}, {"text": "There are three principles involved.", "start": 382.532, "duration": 1.798}, {"text": "The first one is a principle\nof altruism, if you like,", "start": 384.354, "duration": 3.289}, {"text": "that the robot's only objective", "start": 387.667, "duration": 3.262}, {"text": "is to maximize the realization\nof human objectives,", "start": 390.953, "duration": 4.246}, {"text": "of human values.", "start": 395.223, "duration": 1.39}, {"text": "And by values here I don't mean\ntouchy-feely, goody-goody values.", "start": 396.637, "duration": 3.33}, {"text": "I just mean whatever it is\nthat the human would prefer", "start": 399.991, "duration": 3.787}, {"text": "their life to be like.", "start": 403.802, "duration": 1.343}, {"text": "And so this actually violates Asimov's law", "start": 407.184, "duration": 2.309}, {"text": "that the robot has to protect\nits own existence.", "start": 409.517, "duration": 2.329}, {"text": "It has no interest in preserving\nits existence whatsoever.", "start": 411.87, "duration": 3.723}, {"text": "The second law is a law\nof humility, if you like.", "start": 417.24, "duration": 3.768}, {"text": "And this turns out to be really\nimportant to make robots safe.", "start": 421.794, "duration": 3.743}, {"text": "It says that the robot does not know", "start": 425.561, "duration": 3.142}, {"text": "what those human values are,", "start": 428.727, "duration": 2.028}, {"text": "so it has to maximize them,\nbut it doesn't know what they are.", "start": 430.779, "duration": 3.178}, {"text": "And that avoids this problem\nof single-minded pursuit", "start": 435.074, "duration": 2.626}, {"text": "of an objective.", "start": 437.724, "duration": 1.212}, {"text": "This uncertainty turns out to be crucial.", "start": 438.96, "duration": 2.172}, {"text": "Now, in order to be useful to us,", "start": 441.546, "duration": 1.639}, {"text": "it has to have some idea of what we want.", "start": 443.209, "duration": 2.731}, {"text": "It obtains that information primarily\nby observation of human choices,", "start": 447.043, "duration": 5.427}, {"text": "so our own choices reveal information", "start": 452.494, "duration": 2.801}, {"text": "about what it is that we prefer\nour lives to be like.", "start": 455.319, "duration": 3.3}, {"text": "So those are the three principles.", "start": 460.452, "duration": 1.683}, {"text": "Let's see how that applies\nto this question of:", "start": 462.159, "duration": 2.318}, {"text": "\"Can you switch the machine off?\"\nas Turing suggested.", "start": 464.501, "duration": 2.789}, {"text": "So here's a PR2 robot.", "start": 468.893, "duration": 2.12}, {"text": "This is one that we have in our lab,", "start": 471.037, "duration": 1.821}, {"text": "and it has a big red \"off\" switch\nright on the back.", "start": 472.882, "duration": 2.903}, {"text": "The question is: Is it\ngoing to let you switch it off?", "start": 476.361, "duration": 2.615}, {"text": "If we do it the classical way,", "start": 479.0, "duration": 1.465}, {"text": "we give it the objective of, \"Fetch\nthe coffee, I must fetch the coffee,", "start": 480.489, "duration": 3.482}, {"text": "I can't fetch the coffee if I'm dead,\"", "start": 483.995, "duration": 2.58}, {"text": "so obviously the PR2\nhas been listening to my talk,", "start": 486.599, "duration": 3.341}, {"text": "and so it says, therefore,\n\"I must disable my 'off' switch,", "start": 489.964, "duration": 3.753}, {"text": "and probably taser all the other\npeople in Starbucks", "start": 494.796, "duration": 2.694}, {"text": "who might interfere with me.\"", "start": 497.514, "duration": 1.56}, {"text": "(Laughter)", "start": 499.098, "duration": 2.062}, {"text": "So this seems to be inevitable, right?", "start": 501.184, "duration": 2.153}, {"text": "This kind of failure mode\nseems to be inevitable,", "start": 503.361, "duration": 2.398}, {"text": "and it follows from having\na concrete, definite objective.", "start": 505.783, "duration": 3.543}, {"text": "So what happens if the machine\nis uncertain about the objective?", "start": 510.632, "duration": 3.144}, {"text": "Well, it reasons in a different way.", "start": 513.8, "duration": 2.127}, {"text": "It says, \"OK, the human\nmight switch me off,", "start": 515.951, "duration": 2.424}, {"text": "but only if I'm doing something wrong.", "start": 518.964, "duration": 1.866}, {"text": "Well, I don't really know what wrong is,", "start": 521.567, "duration": 2.475}, {"text": "but I know that I don't want to do it.\"", "start": 524.066, "duration": 2.044}, {"text": "So that's the first and second\nprinciples right there.", "start": 526.134, "duration": 3.01}, {"text": "\"So I should let the human switch me off.\"", "start": 529.168, "duration": 3.359}, {"text": "And in fact you can calculate\nthe incentive that the robot has", "start": 533.541, "duration": 3.956}, {"text": "to allow the human to switch it off,", "start": 537.521, "duration": 2.493}, {"text": "and it's directly tied to the degree", "start": 540.038, "duration": 1.914}, {"text": "of uncertainty about\nthe underlying objective.", "start": 541.976, "duration": 2.746}, {"text": "And then when the machine is switched off,", "start": 545.797, "duration": 2.949}, {"text": "that third principle comes into play.", "start": 548.77, "duration": 1.805}, {"text": "It learns something about the objectives\nit should be pursuing,", "start": 550.599, "duration": 3.062}, {"text": "because it learns that\nwhat it did wasn't right.", "start": 553.685, "duration": 2.533}, {"text": "In fact, we can, with suitable use\nof Greek symbols,", "start": 556.242, "duration": 3.57}, {"text": "as mathematicians usually do,", "start": 559.836, "duration": 2.131}, {"text": "we can actually prove a theorem", "start": 561.991, "duration": 1.984}, {"text": "that says that such a robot\nis provably beneficial to the human.", "start": 563.999, "duration": 3.553}, {"text": "You are provably better off\nwith a machine that's designed in this way", "start": 567.576, "duration": 3.803}, {"text": "than without it.", "start": 571.403, "duration": 1.246}, {"text": "So this is a very simple example,\nbut this is the first step", "start": 573.057, "duration": 2.906}, {"text": "in what we're trying to do\nwith human-compatible AI.", "start": 575.987, "duration": 3.903}, {"text": "Now, this third principle,", "start": 582.477, "duration": 3.257}, {"text": "I think is the one that you're probably\nscratching your head over.", "start": 585.758, "duration": 3.112}, {"text": "You're probably thinking, \"Well,\nyou know, I behave badly.", "start": 588.894, "duration": 3.239}, {"text": "I don't want my robot to behave like me.", "start": 592.157, "duration": 2.929}, {"text": "I sneak down in the middle of the night\nand take stuff from the fridge.", "start": 595.11, "duration": 3.434}, {"text": "I do this and that.\"", "start": 598.568, "duration": 1.168}, {"text": "There's all kinds of things\nyou don't want the robot doing.", "start": 599.76, "duration": 2.797}, {"text": "But in fact, it doesn't\nquite work that way.", "start": 602.581, "duration": 2.071}, {"text": "Just because you behave badly", "start": 604.676, "duration": 2.155}, {"text": "doesn't mean the robot\nis going to copy your behavior.", "start": 606.855, "duration": 2.623}, {"text": "It's going to understand your motivations\nand maybe help you resist them,", "start": 609.502, "duration": 3.91}, {"text": "if appropriate.", "start": 613.436, "duration": 1.32}, {"text": "But it's still difficult.", "start": 616.026, "duration": 1.464}, {"text": "What we're trying to do, in fact,", "start": 618.122, "duration": 2.545}, {"text": "is to allow machines to predict\nfor any person and for any possible life", "start": 620.691, "duration": 5.796}, {"text": "that they could live,", "start": 626.511, "duration": 1.161}, {"text": "and the lives of everybody else:", "start": 627.696, "duration": 1.597}, {"text": "Which would they prefer?", "start": 629.317, "duration": 2.517}, {"text": "And there are many, many\ndifficulties involved in doing this;", "start": 633.881, "duration": 2.954}, {"text": "I don't expect that this\nis going to get solved very quickly.", "start": 636.859, "duration": 2.932}, {"text": "The real difficulties, in fact, are us.", "start": 639.815, "duration": 2.643}, {"text": "As I have already mentioned,\nwe behave badly.", "start": 643.969, "duration": 3.117}, {"text": "In fact, some of us are downright nasty.", "start": 647.11, "duration": 2.321}, {"text": "Now the robot, as I said,\ndoesn't have to copy the behavior.", "start": 650.251, "duration": 3.052}, {"text": "The robot does not have\nany objective of its own.", "start": 653.327, "duration": 2.791}, {"text": "It's purely altruistic.", "start": 656.142, "duration": 1.737}, {"text": "And it's not designed just to satisfy\nthe desires of one person, the user,", "start": 659.113, "duration": 5.221}, {"text": "but in fact it has to respect\nthe preferences of everybody.", "start": 664.358, "duration": 3.138}, {"text": "So it can deal with a certain\namount of nastiness,", "start": 669.083, "duration": 2.57}, {"text": "and it can even understand\nthat your nastiness, for example,", "start": 671.677, "duration": 3.701}, {"text": "you may take bribes as a passport official", "start": 675.402, "duration": 2.671}, {"text": "because you need to feed your family\nand send your kids to school.", "start": 678.097, "duration": 3.812}, {"text": "It can understand that;\nit doesn't mean it's going to steal.", "start": 681.933, "duration": 2.906}, {"text": "In fact, it'll just help you\nsend your kids to school.", "start": 684.863, "duration": 2.679}, {"text": "We are also computationally limited.", "start": 688.796, "duration": 3.012}, {"text": "Lee Sedol is a brilliant Go player,", "start": 691.832, "duration": 2.505}, {"text": "but he still lost.", "start": 694.361, "duration": 1.325}, {"text": "So if we look at his actions,\nhe took an action that lost the game.", "start": 695.71, "duration": 4.239}, {"text": "That doesn't mean he wanted to lose.", "start": 699.973, "duration": 2.161}, {"text": "So to understand his behavior,", "start": 703.16, "duration": 2.04}, {"text": "we actually have to invert\nthrough a model of human cognition", "start": 705.224, "duration": 3.644}, {"text": "that includes our computational\nlimitations -- a very complicated model.", "start": 708.892, "duration": 4.977}, {"text": "But it's still something\nthat we can work on understanding.", "start": 713.893, "duration": 2.993}, {"text": "Probably the most difficult part,\nfrom my point of view as an AI researcher,", "start": 717.696, "duration": 4.32}, {"text": "is the fact that there are lots of us,", "start": 722.04, "duration": 2.575}, {"text": "and so the machine has to somehow\ntrade off, weigh up the preferences", "start": 726.114, "duration": 3.581}, {"text": "of many different people,", "start": 729.719, "duration": 2.225}, {"text": "and there are different ways to do that.", "start": 731.968, "duration": 1.906}, {"text": "Economists, sociologists,\nmoral philosophers have understood that,", "start": 733.898, "duration": 3.689}, {"text": "and we are actively\nlooking for collaboration.", "start": 737.611, "duration": 2.455}, {"text": "Let's have a look and see what happens\nwhen you get that wrong.", "start": 740.09, "duration": 3.251}, {"text": "So you can have\na conversation, for example,", "start": 743.365, "duration": 2.133}, {"text": "with your intelligent personal assistant", "start": 745.522, "duration": 1.944}, {"text": "that might be available\nin a few years' time.", "start": 747.49, "duration": 2.285}, {"text": "Think of a Siri on steroids.", "start": 749.799, "duration": 2.524}, {"text": "So Siri says, \"Your wife called\nto remind you about dinner tonight.\"", "start": 753.447, "duration": 4.322}, {"text": "And of course, you've forgotten.\n\"What? What dinner?", "start": 758.436, "duration": 2.508}, {"text": "What are you talking about?\"", "start": 760.968, "duration": 1.425}, {"text": "\"Uh, your 20th anniversary at 7pm.\"", "start": 762.417, "duration": 3.746}, {"text": "\"I can't do that. I'm meeting\nwith the secretary-general at 7:30.", "start": 768.735, "duration": 3.719}, {"text": "How could this have happened?\"", "start": 772.478, "duration": 1.692}, {"text": "\"Well, I did warn you, but you overrode\nmy recommendation.\"", "start": 774.194, "duration": 4.66}, {"text": "\"Well, what am I going to do?\nI can't just tell him I'm too busy.\"", "start": 779.966, "duration": 3.328}, {"text": "\"Don't worry. I arranged\nfor his plane to be delayed.\"", "start": 784.31, "duration": 3.281}, {"text": "(Laughter)", "start": 787.615, "duration": 1.682}, {"text": "\"Some kind of computer malfunction.\"", "start": 790.069, "duration": 2.101}, {"text": "(Laughter)", "start": 792.194, "duration": 1.212}, {"text": "\"Really? You can do that?\"", "start": 793.43, "duration": 1.617}, {"text": "\"He sends his profound apologies", "start": 796.22, "duration": 2.179}, {"text": "and looks forward to meeting you\nfor lunch tomorrow.\"", "start": 798.423, "duration": 2.555}, {"text": "(Laughter)", "start": 801.002, "duration": 1.299}, {"text": "So the values here --\nthere's a slight mistake going on.", "start": 802.325, "duration": 4.403}, {"text": "This is clearly following my wife's values", "start": 806.752, "duration": 3.009}, {"text": "which is \"Happy wife, happy life.\"", "start": 809.785, "duration": 2.069}, {"text": "(Laughter)", "start": 811.878, "duration": 1.583}, {"text": "It could go the other way.", "start": 813.485, "duration": 1.444}, {"text": "You could come home\nafter a hard day's work,", "start": 815.641, "duration": 2.201}, {"text": "and the computer says, \"Long day?\"", "start": 817.866, "duration": 2.195}, {"text": "\"Yes, I didn't even have time for lunch.\"", "start": 820.085, "duration": 2.288}, {"text": "\"You must be very hungry.\"", "start": 822.397, "duration": 1.282}, {"text": "\"Starving, yeah.\nCould you make some dinner?\"", "start": 823.703, "duration": 2.646}, {"text": "\"There's something I need to tell you.\"", "start": 827.89, "duration": 2.09}, {"text": "(Laughter)", "start": 830.004, "duration": 1.155}, {"text": "\"There are humans in South Sudan\nwho are in more urgent need than you.\"", "start": 832.013, "duration": 4.905}, {"text": "(Laughter)", "start": 836.942, "duration": 1.104}, {"text": "\"So I'm leaving. Make your own dinner.\"", "start": 838.07, "duration": 2.075}, {"text": "(Laughter)", "start": 840.169, "duration": 2.0}, {"text": "So we have to solve these problems,", "start": 842.643, "duration": 1.739}, {"text": "and I'm looking forward\nto working on them.", "start": 844.406, "duration": 2.515}, {"text": "There are reasons for optimism.", "start": 846.945, "duration": 1.843}, {"text": "One reason is,", "start": 848.812, "duration": 1.159}, {"text": "there is a massive amount of data.", "start": 849.995, "duration": 1.868}, {"text": "Because remember -- I said\nthey're going to read everything", "start": 851.887, "duration": 2.794}, {"text": "the human race has ever written.", "start": 854.705, "duration": 1.546}, {"text": "Most of what we write about\nis human beings doing things", "start": 856.275, "duration": 2.724}, {"text": "and other people getting upset about it.", "start": 859.023, "duration": 1.914}, {"text": "So there's a massive amount\nof data to learn from.", "start": 860.961, "duration": 2.398}, {"text": "There's also a very\nstrong economic incentive", "start": 863.383, "duration": 2.236}, {"text": "to get this right.", "start": 867.151, "duration": 1.186}, {"text": "So imagine your domestic robot's at home.", "start": 868.361, "duration": 2.001}, {"text": "You're late from work again\nand the robot has to feed the kids,", "start": 870.386, "duration": 3.067}, {"text": "and the kids are hungry\nand there's nothing in the fridge.", "start": 873.477, "duration": 2.823}, {"text": "And the robot sees the cat.", "start": 876.324, "duration": 2.605}, {"text": "(Laughter)", "start": 878.953, "duration": 1.692}, {"text": "And the robot hasn't quite learned\nthe human value function properly,", "start": 880.669, "duration": 4.19}, {"text": "so it doesn't understand", "start": 884.883, "duration": 1.251}, {"text": "the sentimental value of the cat outweighs\nthe nutritional value of the cat.", "start": 886.158, "duration": 4.844}, {"text": "(Laughter)", "start": 891.026, "duration": 1.095}, {"text": "So then what happens?", "start": 892.145, "duration": 1.748}, {"text": "Well, it happens like this:", "start": 893.917, "duration": 3.297}, {"text": "\"Deranged robot cooks kitty\nfor family dinner.\"", "start": 897.238, "duration": 2.964}, {"text": "That one incident would be the end\nof the domestic robot industry.", "start": 900.226, "duration": 4.523}, {"text": "So there's a huge incentive\nto get this right", "start": 904.773, "duration": 3.372}, {"text": "long before we reach\nsuperintelligent machines.", "start": 908.169, "duration": 2.715}, {"text": "So to summarize:", "start": 911.948, "duration": 1.535}, {"text": "I'm actually trying to change\nthe definition of AI", "start": 913.507, "duration": 2.881}, {"text": "so that we have provably\nbeneficial machines.", "start": 916.412, "duration": 2.993}, {"text": "And the principles are:", "start": 919.429, "duration": 1.222}, {"text": "machines that are altruistic,", "start": 920.675, "duration": 1.398}, {"text": "that want to achieve only our objectives,", "start": 922.097, "duration": 2.804}, {"text": "but that are uncertain\nabout what those objectives are,", "start": 924.925, "duration": 3.116}, {"text": "and will watch all of us", "start": 928.065, "duration": 1.998}, {"text": "to learn more about what it is\nthat we really want.", "start": 930.087, "duration": 3.203}, {"text": "And hopefully in the process,\nwe will learn to be better people.", "start": 934.193, "duration": 3.559}, {"text": "Thank you very much.", "start": 937.776, "duration": 1.191}, {"text": "(Applause)", "start": 938.991, "duration": 3.709}, {"text": "Chris Anderson: So interesting, Stuart.", "start": 942.724, "duration": 1.868}, {"text": "We're going to stand here a bit\nbecause I think they're setting up", "start": 944.616, "duration": 3.17}, {"text": "for our next speaker.", "start": 947.81, "duration": 1.151}, {"text": "A couple of questions.", "start": 948.985, "duration": 1.538}, {"text": "So the idea of programming in ignorance\nseems intuitively really powerful.", "start": 950.547, "duration": 5.453}, {"text": "As you get to superintelligence,", "start": 956.024, "duration": 1.594}, {"text": "what's going to stop a robot", "start": 957.642, "duration": 2.258}, {"text": "reading literature and discovering\nthis idea that knowledge", "start": 959.924, "duration": 2.852}, {"text": "is actually better than ignorance", "start": 962.8, "duration": 1.572}, {"text": "and still just shifting its own goals\nand rewriting that programming?", "start": 964.396, "duration": 4.218}, {"text": "Stuart Russell: Yes, so we want\nit to learn more, as I said,", "start": 969.512, "duration": 6.356}, {"text": "about our objectives.", "start": 975.892, "duration": 1.287}, {"text": "It'll only become more certain\nas it becomes more correct,", "start": 977.203, "duration": 5.521}, {"text": "so the evidence is there", "start": 982.748, "duration": 1.945}, {"text": "and it's going to be designed\nto interpret it correctly.", "start": 984.717, "duration": 2.724}, {"text": "It will understand, for example,\nthat books are very biased", "start": 987.465, "duration": 3.956}, {"text": "in the evidence they contain.", "start": 991.445, "duration": 1.483}, {"text": "They only talk about kings and princes", "start": 992.952, "duration": 2.397}, {"text": "and elite white male people doing stuff.", "start": 995.373, "duration": 2.8}, {"text": "So it's a complicated problem,", "start": 998.197, "duration": 2.096}, {"text": "but as it learns more about our objectives", "start": 1000.317, "duration": 3.872}, {"text": "it will become more and more useful to us.", "start": 1004.213, "duration": 2.063}, {"text": "CA: And you couldn't\njust boil it down to one law,", "start": 1006.3, "duration": 2.526}, {"text": "you know, hardwired in:", "start": 1008.85, "duration": 1.65}, {"text": "\"if any human ever tries to switch me off,", "start": 1010.524, "duration": 3.293}, {"text": "I comply. I comply.\"", "start": 1013.841, "duration": 1.935}, {"text": "SR: Absolutely not.", "start": 1015.8, "duration": 1.182}, {"text": "That would be a terrible idea.", "start": 1017.006, "duration": 1.499}, {"text": "So imagine that you have\na self-driving car", "start": 1018.529, "duration": 2.689}, {"text": "and you want to send your five-year-old", "start": 1021.242, "duration": 2.433}, {"text": "off to preschool.", "start": 1023.699, "duration": 1.174}, {"text": "Do you want your five-year-old\nto be able to switch off the car", "start": 1024.897, "duration": 3.101}, {"text": "while it's driving along?", "start": 1028.022, "duration": 1.213}, {"text": "Probably not.", "start": 1029.259, "duration": 1.159}, {"text": "So it needs to understand how rational\nand sensible the person is.", "start": 1030.442, "duration": 4.703}, {"text": "The more rational the person,", "start": 1035.169, "duration": 1.676}, {"text": "the more willing you are\nto be switched off.", "start": 1036.869, "duration": 2.103}, {"text": "If the person is completely\nrandom or even malicious,", "start": 1038.996, "duration": 2.543}, {"text": "then you're less willing\nto be switched off.", "start": 1041.563, "duration": 2.512}, {"text": "CA: All right. Stuart, can I just say,", "start": 1044.099, "duration": 1.866}, {"text": "I really, really hope you\nfigure this out for us.", "start": 1045.989, "duration": 2.314}, {"text": "Thank you so much for that talk.\nThat was amazing.", "start": 1048.327, "duration": 2.375}, {"text": "SR: Thank you.", "start": 1050.726, "duration": 1.167}, {"text": "(Applause)", "start": 1051.917, "duration": 1.837}]